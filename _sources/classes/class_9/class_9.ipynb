{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 9: Advanced and Performant Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the best things about Python is how easy it is to get started with. The syntax is clear, it has all the basic features, things work in a relatively intuitive way, life is good. But Python also supports very advanced features, which make coding with Python an enjoyable experience even after you think you've learned everything there is to know of the language. You can always dive a little deeper. You're not required to, but you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simplistic terms, generators are iterators. Meaning, a generator is always an object you can iterate over. In Python you can iterate over most data structures, including dictionaries, lists, tuples and more - and so in this sense generators are similar. However, when we iterate over a list, for example, we're iterating over an existing data structures with existing items. Same goes for dictionaries, when we iterate over them, Python \"hands over\" the dictionary's keys and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "a_list = [0, 10, 20]\n",
    "for item in a_list:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0\n",
      "b 10\n",
      "c 20\n"
     ]
    }
   ],
   "source": [
    "a_dict = dict(a=0, b=10, c=20)\n",
    "for key in a_dict:\n",
    "    print(key, a_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first major difference between a generator and the other iterators. A generator is a _recipe_ to create the next item in the chain. A generator is a piece of code telling the Python interpreter how to create the next item, but it doesn't hold this item in memory yet. A simple example might be a list containing values from 0 to 1000. A generator of this list will not have 1000 cells with their values - it would have instructions on the number of cells, and how to calculate the next value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already met a kind of generator: the `range()` function. When we tell Python to give us a range of number between 0 and 1000 by writing `range(1000)` - we're not actually generating the 1000 \"cells\" of values, but only the recipe. Let's see it in \"action\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 1000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(1000)  # a \"range\" object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 1000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = range(1000)\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8056"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(list(items)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple 1000-element list isn't that heavy for a computer (even an Arduino or similar), but when lists get longer, with bigger arrays and massive data structures inside them, it's very inefficient to hold this amount of unused data in memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Creation and Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our own generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_range(n):\n",
    "    \"\"\"\n",
    "    Returns a list of items from 0 to n.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Number of items to generate\n",
    "    \"\"\"\n",
    "    num = 0\n",
    "    while num < n:\n",
    "        yield num\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we create a generator, the code is executed until the first `yield` statement. This reserved keyword is what makes a function into a generator.\n",
    "\n",
    "When the code reaches the `yield` it holds, or \"saves\" its current state, until called by Python's `next()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-dd54763376f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "new_range = my_range(3)\n",
    "\n",
    "print(next(new_range))\n",
    "\n",
    "print(next(new_range))\n",
    "\n",
    "print(next(new_range))\n",
    "\n",
    "print(next(new_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time `next()` is used, the line with the `yield` is executed, and the function keeps going until it reaches  another `yield` statement. In the `my_range` function, while the index is smaller than `n` the code will reach a `yield`. When we don't satisfy this condition anymore, the code skips the loop and reaches the end of the function. This results in a special `StopIteration` exception, used only in these special cases. This means you can catch this exception and know that your generator went through all of its items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But calling `next()` multiple times isn't practical. Luckily, `for` loops implement this exact interface automatically, allowing us to use them instead of the tedious, repetitive `next()` calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop_range = my_range(10)\n",
    "\n",
    "for item in loop_range:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `for` loop is also smart enough to catch the `StopIteration` exception and terminate the loop, without raising any \"visible\" exceptions. A `for` loop is the common way to iterate over generators.\n",
    "\n",
    "Note that \"holding off\" the generation of the values means we can't access the complete data structure. If we try to print it, we will simply get the generator instance's representation string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range2 = my_range(10)\n",
    "print(range2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same applies for indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range2[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once used, generators are \"depleted\", you can't reuse them. This is a major difference between a generator and a list. For example, you're not limited by the number of times you can iterate over a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in loop_range:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a `for` loop doesn't return anything, because we already depleted `loop_range`.\n",
    "\n",
    "Let's try something more explicit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(loop_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to stress that all functions can become generators if they contain the `yield` statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stuff():\n",
    "    print(\"Hello, \")\n",
    "    yield True\n",
    "    print(\"World\")\n",
    "    yield False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff_printer = print_stuff()\n",
    "stuff = next(stuff_printer)\n",
    "print(stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_stuff = next(stuff_printer)\n",
    "print(more_stuff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to create generators is \"genexps\", or generator expressions, which are very similar to list comprehensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = (2 * n for n in range(10))\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in nums:\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The round brackets (`()`) tell the interpreter that we're creating a generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When should we use generators? Many libraries use them almost ubiquitously. For example, `pathlib` uses them to iterate over the content of directories - it's not \"cheap\" to get the full directory's content and then iterate over it, so `pathlib` uses a generator to yield the next item every time.\n",
    "\n",
    "In our field of work, generators can either be great to have in order to improve the performance of your code, or an absolute necessity if you're working with very large data structures that simply cannot be handled in-memory. In any case, implementing and using generators is easy and incredibly beneficial for various computations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`````{admonition} Exercise: Fibonacci Generator\n",
    "Write a generator function returning the first `n` Fibonacci numbers.\n",
    "````{dropdown} Solution\n",
    "```{code-block}\n",
    "def generate_n_fibonacci(n):\n",
    "    \"\"\"\n",
    "    Generates the first n Fibonacci numbers.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Length of the generated Fibonacci sequence\n",
    "    \"\"\"\n",
    "    index, a, b = 0, 0, 1\n",
    "    while index < n:\n",
    "        yield a\n",
    "        a, b = b, a + b\n",
    "        index += 1\n",
    "```\n",
    "````\n",
    "*Bonus: Modify your solution to create an infinite Fibonacci generator (infinite in the sense that as long as iteration over it is continued, the next number will be generated).*\n",
    "````{dropdown} Solution\n",
    "```{code-block}\n",
    "def generate_fibonacci():\n",
    "    \"\"\"\n",
    "    Fibonacci sequence generator.\n",
    "    \"\"\"\n",
    "    a, b = 0, 1\n",
    "    while True:\n",
    "        yield a\n",
    "        a, b = b, a + b\n",
    "```\n",
    "````\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `*args`, `**kwargs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You use `*args` and `**kwargs` when you would like to enable a flexible number of arguments that may be passed to a function. Actually, the syntax is only `*` and `**` - the words `args` and `kwargs` are simply used by convention. `args` is obviously arguments, or unnamed arguments given to a function, and `kwargs` is keyword arguments, or arguments given as `key=value` pairs. Let's see a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(required_argument, *args, **kwargs):\n",
    "    print(required_argument)\n",
    "\n",
    "    if args:\n",
    "        print(\"I found something in args!\")\n",
    "        print(args)\n",
    "    \n",
    "    if kwargs:\n",
    "        print(\"I found something in kwargs!\")\n",
    "        for key, value in kwargs.items():\n",
    "            print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "f() missing 1 required positional argument: 'required_argument'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-06d6587ee42e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# doesn't work - we have one required argument to the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: f() missing 1 required positional argument: 'required_argument'"
     ]
    }
   ],
   "source": [
    "f()  # doesn't work - we have one required argument to the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f('required')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f('required', 1, 2, 3)  # the second printed row is the args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f('required', 1, 2, 3, kw1='a', kw2='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `args` is a tuple containing all unnamed parameters that were given to the function in the order they were given, and `kwargs` is a dictionary.\n",
    "\n",
    "Essentially, using `*` in the function's signature is the inverse of using it for expansion when calling the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a=1, b=2):\n",
    "    print(a, b)\n",
    "\n",
    "inputs = {'a': 10, 'b': 20}\n",
    "f(**inputs)\n",
    "\n",
    "def f2(a, b):\n",
    "    print(a, b)\n",
    "\n",
    "my_inputs = (1, 2)\n",
    "f2(*my_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see here is that the function's signature doesn't have to contain `*args` or `**kwargs`. The `**` operator \"opens up\" the input dictionary, allowing the `f()` function to use the parameters without any issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decorators are functions that receive functions in their arguments. When you wrap an existing function with another function, you're creating a decorator. This feature is extensively used in web frameworks, pytest and in other important Python use cases, which means it has a special syntax: `@decorator`. Let's look at an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume I have a large data-processing pipeline script, built out of many smaller functions, which unfortunately takes a long time to run. I wish to understand _why_ it's taking so long, so I decide to add a printed statement at the start and end of each function, so that I could see with my eyes where the code \"hangs\". This is how I implemented it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(fname):\n",
    "    data = load_data(fname)\n",
    "    processed = process_data(data)\n",
    "    appended = append_data(processed)\n",
    "    logged = log_data(appended)\n",
    "\n",
    "def load_data(fname):\n",
    "    print(\"Starting 'load_data'...\")\n",
    "    # ... Code ...\n",
    "    print(\"Ending 'load_data'...\")\n",
    "\n",
    "def process_data(data):\n",
    "    print(\"Starting 'process_data'...\")\n",
    "    # ... Code ...\n",
    "    print(\"Ending 'process_data'...\")\n",
    "    \n",
    "# And so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is obviously very tedious. Even when I only have four functions, it's very repetitive and feels wrong. Moreover, it might have not solved my issue. Let's say my manual examination showed that all four functions take a considerable time to run, so I decide to profile the execution time of each function, to better understand which function is the most costly and optimize it first.\n",
    "\n",
    "Here's how I redefined all functions to measure their execution time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def load_data(fname):\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting 'load_data' at {start_time}...\")    \n",
    "    # ... Code ...\n",
    "    end_time = time.time()\n",
    "    print(f\"Ended 'load_data' at {end_time}...\")\n",
    "    duration = end_time - start_time\n",
    "    print(f\"It took the code {duration} milliseconds to run.\")    \n",
    "\n",
    "    \n",
    "def process_data(data):\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting 'process_data' at {start_time}...\")    \n",
    "    # ... Code ...\n",
    "    end_time = time.time()\n",
    "    print(f\"Ended 'process_data' at {end_time}...\")\n",
    "    duration = end_time - start_time\n",
    "    print(f\"It took the code {duration} milliseconds to run.\")    \n",
    "    \n",
    "# And so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works, but again, it's very repetitive. Also, if I decide that I want to see the execution time in seconds, and not milliseconds, I have to go through each function and re-implement it. Very tedious. \n",
    "\n",
    "Consider the following solution instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printer(func):\n",
    "    def inner_func(a, b):\n",
    "        print(f\"Starting {func.__name__}...\")\n",
    "        result = func(a, b)\n",
    "        print(f\"Ending {func.__name__}...\")\n",
    "        return result\n",
    "    return inner_func      \n",
    "\n",
    "\n",
    "def timer(func):\n",
    "    def inner_func(argument):\n",
    "        start_time = time.time()\n",
    "        result = func(argument)\n",
    "        print(f\"It tooks the code {time.time() - start_time} milliseconds to run.\")\n",
    "        return result\n",
    "    return inner_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks complex at first, but it's really pretty simple. It uses the fact that functions in Python are objects, like any other element in the language. And because they're objects, they can be passed around as arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(func):\n",
    "    \"\"\" Runs the func functions and prints 'hi' at the end \"\"\"\n",
    "    func()\n",
    "    print(\"hi\")\n",
    "    \n",
    "def print_hello():\n",
    "    print(\"hello\")\n",
    "    \n",
    "\n",
    "f(print_hello)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like all objects, functions have attributes. Namely, they have the `__name__` attribute which contains... their name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f.__name__)\n",
    "print(print_hello.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know we can pass functions as arguments to other functions. Let's try to examine the `printer` and `timer` functions again.\n",
    "\n",
    "They're both a function that receives a different, \"unknown\" function, as its argument. So far, so good. Then it defines another function which \"wraps\" the original function with some actions, like printing or timing. This inner function runs the original function and returns the result. In essence, it created a \"new implementation\" of that original function that does the exact same thing, but with the wrapping functionality (printing, timing, etc.). This new function (`inner_func`) can replace any instance of the original function without any troubles, since in essence it just calls it. It's adds a couple of statements before and after that call, but the essential functionality remained unchanged.\n",
    "\n",
    "Lastly, the outer function, which we call the decorator, returns the inner function as its return value. So this function receives a function as its argument and return a new, improved function as its output. To use it, we just \"rename\" the existing functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_printer = printer(load_data)\n",
    "load_data_timed = timer(load_data)\n",
    "\n",
    "process_data_printer = printer(process_data)\n",
    "process_data_timed = timer(process_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obviously use this `timer` function on any function we wish to time. When we wish to time functions in seconds, rather than milliseconds, we'll just change this one instance of `timer` and be done with it, and likewise for `printer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only small caveat here is the fact that we currently require the function we're replacing to have a single `argument` as its argument. This implementation detail is small but very impactful - it means that our decorator will only decorate successfully functions that have a single argument. To remedy this we'll have to use `*args` and `**kwargs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printer(func):\n",
    "    def inner_func(*args, **kwargs):\n",
    "        print(f\"Starting {func.__name__}...\")\n",
    "        result = func(*args, **kwargs)\n",
    "        print(f\"Ending {func.__name__}...\")\n",
    "        return result\n",
    "    return inner_func      \n",
    "\n",
    "\n",
    "def timer(func):\n",
    "    def inner_func(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        print(f\"It tooks the code {time.time() - start_time} milliseconds to run.\")\n",
    "        return result\n",
    "    return inner_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now be sure that our functions will always run regardless the number of inputs given to them. However, we still have to redefine all functions as we've seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_printer = printer(load_data)\n",
    "load_data_timed = timer(load_data)\n",
    "\n",
    "process_data_printer = printer(process_data)\n",
    "process_data_timed = timer(process_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still requires us to rename all instances of these functions in all places of the code, and when we're done with the printing and timing, we have to rename them back.\n",
    "\n",
    "Why not \"rename\" the function back to its original name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data = printer(load_data)\n",
    "process_data = timer(process_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This idiom is common enough to have a built-in language syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def load_data(fname):\n",
    "    # ... Code ...\n",
    "    pass\n",
    "\n",
    "load_data('fname.txt')\n",
    "load_data(fname='fname.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use multiple decorators for functions as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@printer\n",
    "@timer\n",
    "def process_data(data, shape):\n",
    "    # ... Code ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we wish to stop printing and timing our function, we simply delete this decorator in the relevant places.\n",
    "\n",
    "Decorators allow more complex calls, like calling them with arguments, but we'll leave that topic for another day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Useful Built-in Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python standard library comes with a number of [built-in modules](https://docs.python.org/3/py-modindex.html) that can make your life much easier. As (neuroscience-oriented) data scientists of sorts, you'll save yourself a lot of hassle by familiarizing yourself with some, namely `collections` and `itertools`. Let's explore a couple of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `collections`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `namedtuple`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want a tiny object with named fields, but without the hassle of creating a fully-fledged class, you actually wish to generate a `namedtuple`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point(x=0, y=1)\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Point = namedtuple('Point', ['x', 'y'])\n",
    "p1 = Point(0, 0)\n",
    "p2 = Point(x=0, y=1)\n",
    "p3 = Point(1, y=2)\n",
    "\n",
    "print(p2)\n",
    "print(p3.y)\n",
    "print(p1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the data inside a `namedtuple` using either the positional index (`[0]`) or the name of that field (`x`). If all you wish to do is to a keep a small record of something, `namedtuple` is your best option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `defaultdict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `defaultdict` is a dictionary that resorts to execute a predefined function if it doesn't find the key. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'three'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d27b12743133>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtwo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'one'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'three'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'three'"
     ]
    }
   ],
   "source": [
    "d = dict(one=1, two=2)\n",
    "print(d['one'])\n",
    "print(d['three'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than a `KeyError`, a `defaultdict` would run a predefined function instead of raising this exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {'one': 1, 'two': 2})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "d2 = defaultdict(list, one=1, two=2)\n",
    "d2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when we call it with an unknown key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(d2['one'])\n",
    "print(d2['three'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It used the `list` \"factory\" to create a new list in that key. This is particularly useful when sorting some key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {'yellow': [1, 3], 'blue': [2, 4], 'red': [1]})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)]\n",
    "d2 = defaultdict(list)\n",
    "for k, v in s:\n",
    "    d2[k].append(v)\n",
    "\n",
    "d2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Itertools`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chained iterables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wish to iterate over several iterables together, we can use the following method from the `itertools` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "e\n",
      "f\n",
      "g\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "chained = itertools.chain('abcd', 'efg')\n",
    "for letter in chained:\n",
    "    print(letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or from an iterable of iterables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "chained_2 = itertools.chain.from_iterable([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "for number in chained_2:\n",
    "    print(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `itertools` always creates generators from the items it receives as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'B'),\n",
       " ('A', 'C'),\n",
       " ('A', 'D'),\n",
       " ('B', 'A'),\n",
       " ('B', 'C'),\n",
       " ('B', 'D'),\n",
       " ('C', 'A'),\n",
       " ('C', 'B'),\n",
       " ('C', 'D'),\n",
       " ('D', 'A'),\n",
       " ('D', 'B'),\n",
       " ('D', 'C')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.permutations('ABCD', 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'B'), ('A', 'C'), ('A', 'D'), ('B', 'C'), ('B', 'D'), ('C', 'D')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.combinations('ABCD', 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're looking for more advanced iteration recipes, like chunking, running windows and more, take a look at [more-itertools](https://pypi.org/project/more-itertools/) package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to utilize parallel processing in Python. The easiest of all is multi-processing, i.e. the use of several CPU cores to run jobs in parallel. This is best used when each process is independent from the others, not having to share data between them. \n",
    "\n",
    "A typical use case is when we have a list holding data, or filenames to where the data is, and we wish to perform the same computation on each element of that list. If this computation is truly independent, the `multiprocessing` module has some very easy-to-use solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import multiprocessing\n",
    "\n",
    "def add_tuple(tup):\n",
    "    return tup[0] + tup[1]\n",
    "\n",
    "tups = [(0, 1), (2, 3), (4, 5), (6, 7)]\n",
    "with multiprocessing.Pool() as pool:  # can also enter the number of processes you wish to use\n",
    "    result = pool.map(add_tuple, tups)\n",
    "result  # [1, 5, 9, 13]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above doesn't work in IPython and Jupyter (see [`ipyparallel`](https://github.com/ipython/ipyparallel) for that purpose), but the general idea of using parallel processing in Python is usually something along these lines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threading is Python's weak point because of the [GIL](https://wiki.python.org/moin/GlobalInterpreterLock), and we'll not discuss it in this class. Another form of parallel processing is asynchronous programming, which we'll also not cover, but is actually one of Python's strongest points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numba` is a special library designed to speed-up computations in Python. Let's jump right into it and then discuss some of the magic afterwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@jit\n",
    "def sum2d(arr):\n",
    "    M, N = arr.shape\n",
    "    result = 0.0\n",
    "    for i in range(M):\n",
    "        for j in range(N):\n",
    "            result += arr[i,j]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy:\n",
      "43.2 ms ± 298 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Numba:\n",
      "98.1 ms ± 299 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "arr = np.ones((10000, 10000))\n",
    "\n",
    "print(\"Numpy:\")\n",
    "%timeit arr.sum()\n",
    "print(\"Numba:\")\n",
    "%timeit sum2d(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results up there are, for lack of a better word, amazing. Numpy has been optimized for ages, works in bare C, and still only offers a mild improvement over `numba`, which seemingly just decorates a simple, perhaps _simplistic_, Python loop, making it amazingly fast.\n",
    "\n",
    "This magic happens with [LLVM](https://llvm.org/), an open-source project that aims at building a very fast, cross-language compiler. `numba` translates the Python code into LLVM-suitable code and lets it take care of the optimization details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba has more tricks in its sleeve. You can define the input types and specify [`nopython=True`](https://numba.pydata.org/numba-doc/latest/user/5minguide.html#what-is-nopython-mode) push performance even further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit, float64\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@jit(float64(float64[:, :]), nopython=True)\n",
    "def sum2d_inps(arr):\n",
    "    M, N = arr.shape\n",
    "    result = np.float64(0.0)\n",
    "    for i in range(M):\n",
    "        for j in range(N):\n",
    "            result += arr[i,j]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy:\n",
      "43.3 ms ± 871 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Numba:\n",
      "98.1 ms ± 470 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"Numpy:\")\n",
    "%timeit arr.sum()\n",
    "print(\"Numba:\")\n",
    "%timeit sum2d_inps(arr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use parallel looping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@njit([float64(float64[:, :])], parallel=True) # njit is equivalent to jit with nopython=True\n",
    "def sum2d_p(arr):\n",
    "    M, N = arr.shape\n",
    "    result = np.float64(0.0)\n",
    "    for i in prange(M): # Note range was replaced with prange\n",
    "        for j in prange(N):\n",
    "            result += arr[i,j]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.3 ms ± 1.11 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sum2d_p(arr)  # pretty cool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing long computations heavily based on `numpy` and iterations, `numba` might be the way to go. For very complicated functions that use fancy linear algebra algorithms, it might be the case that `numba` doesn't support these methods yet. In these occasions resort to basic `numpy` functions and wait till the `numba` developers implement that method, or do so yourself! `numba` is completely open-sourced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you wish to write performant code that utilizes significant parts of the standard library, as well as `numpy` and the scientific stack, neither `numpy` nor `numba` will help you. They require that you work with arrays, which are not as easy to work with as lists, for example. Dictionaries are also very helpful, but using them only with the standard Python interpreter will hinder you performance considerably.\n",
    "\n",
    "These are the cases where Cython shines. It allows you to write code with Python-like syntax and compile it ahead-of-time to a `myfile.c` source file, written in `C` automatically. When your code calls a function that was written in Cython, it will actually turn to the optimized `C` function and use that function instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated, Cython requires you to compile your code before running the parent Python script. To do that, you have to create a `setup.py` file that tells the Cython compiler where to find the files in question.\n",
    "\n",
    "A Cython file ends with `X.pyx`, so `setup.py` should point there. Here's a basic example of `setup.py`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from distutils.core import setup\n",
    "from Cython.Build import cythonize\n",
    "\n",
    "setup(\n",
    "    ext_modules = cythonize('my_file.pyx'),\n",
    "    # other setup.py options come here\n",
    ")\n",
    "```\n",
    "\n",
    "Then you navigate with your command line to the folder containing `setup.py` and write `python setup.py build_ext --inplace`, which tells Cython to \"build\", i.e. compile, the code in the `.pyx` file and add it `inplace`, i.e. to this directory.\n",
    "\n",
    "An example can be found in the `cython_demo` folder. Let's see it here in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pyximport; pyximport.install(language_level=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from cython_demo import plain_python\n",
    "from cython_demo import primes_cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_python.primes_python(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primes_cython.primes(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.6 ms ± 54.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "2.46 ms ± 77.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit plain_python.primes_python(1000)\n",
    "%timeit primes_cython.primes(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare NumPy's basic performance with the same functionality implemented with `numba` or `cython`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rands = np.random.random((1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.87 ms ± 81.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rands[rands < 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def filter_larger(rands):\n",
    "    arr = np.zeros_like(rands)\n",
    "    thresh = 0.5\n",
    "    last_idx = 0\n",
    "    for idx in prange(len(rands)):\n",
    "        if rands[idx] < 0.5:\n",
    "            arr[last_idx] = rands[idx]\n",
    "            last_idx += 1\n",
    "            \n",
    "    return arr[:last_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.87 ms ± 432 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit filter_larger(rands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that the `last_idx` variable is probably hindering performance of the parallel loop.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cython:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cython_filter_demo import filter_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.23 ms ± 66.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit filter_array.filter_larger_cython(rands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memoization (Caching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another way to improve performance of your scripts, perhaps a more straight-forward one, is memoization. This essentially means caching (saving) the results of computations done for a given set of parameters. Every time the function is called it first checks whether the result of the operation was already computed earlier, and if so it immediately returns it rather than re-computing it all over again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching is extremely easy to do in Python. The standard library includes a module called `functools` which contains several important functions that work on other functions, and one of them is `lru_cache`, which stands for \"least recently used\". While it's not the only way to do memoization in Python (there are multiple 3rd partly libraries that implement fancy memoization techniques), `lru_cache` is usually good enough.\n",
    "\n",
    "Using it is extremely simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib(n: int) -> int:\n",
    "    \"\"\"\n",
    "    Returns the *n*th Fibonacci sequence element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Index of the desired Fibonacci number\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The value of the *n*th Fibonacci number\n",
    "    \"\"\"\n",
    "    if n < 2:\n",
    "        return n\n",
    "    return fib(n-1) + fib(n-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fibonacci series is a classic example, since every computation of a new element in the sequence is built on previous calculations. The function above is a simple implementation using recursion, but it currently doesn't cache its result. Meaning that it has to re-compute all values whenever its called.\n",
    "\n",
    "To cache the result we simply have to add a decorator to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "\n",
    "@functools.lru_cache()\n",
    "def fib(n: int) -> int:\n",
    "    \"\"\"\n",
    "    Returns the *n*th Fibonacci sequence element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Index of the desired Fibonacci number\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The value of the *n*th Fibonacci number\n",
    "    \"\"\"\n",
    "    if n < 2:\n",
    "        return n\n",
    "    return fib(n-1) + fib(n-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the timings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.2 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n1 -r1 fib(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.62 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n1 -r1 fib(61)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running `fib(61)` takes almost no time, since the result of `fib(60)` is already cached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Code Smells\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now turn our attention to higher-minded concepts that you should pay attention to when creating software. The term above refers to elements in your code base that represent something which is not _wrong_, but can probably be better. That's what the \"smell\" means - it's like a bad feeling about the code, but it's not something which will tear down your application if it remains as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repetitive code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rule of thumb here is once you understand that some piece of code will be re-used somewhere else, immediately extract it out into a function and call that function instead. This will help you test it for correctness, document it more thoroughly and improve the readability of the piece of code using this new function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following made-up snippets from some fantastic analysis:\n",
    "\n",
    "```{code-block}\n",
    "import tifffile\n",
    "\n",
    "\n",
    "# ...\n",
    "# In the middle of some data analysis script\n",
    "file_list_type_1 = ['a.tif', 'b.tif', 'c.tif']\n",
    "for file in file_list_type_1:\n",
    "    img = tifffile.imread(file)\n",
    "    img -= img.min()\n",
    "    img /= img.max()\n",
    "\n",
    "# ...\n",
    "file_list_type_2 = ['x.tif', 'y.tif', 'z.tif']\n",
    "for file in file_list_type_2:\n",
    "    img = tifffile.imread(file)\n",
    "    img -= img.min()\n",
    "    img /= img.max()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can definitely smell something. Let's try:\n",
    "\n",
    "```{code-block}\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def normalize_image(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" \n",
    "    Receives an image in the form of a numpy array, makes \n",
    "    it positive and normalizes it between 0 and 1.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        Image to be normalized\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Normalized image\n",
    "    \"\"\"\n",
    "    img -= img.min()\n",
    "    img /= img.max()\n",
    "    return img\n",
    "\n",
    "# ...\n",
    "file_list_type_1 = ['a.tif', 'b.tif', 'c.tif']\n",
    "for file in file_list_type_1:\n",
    "    img = tifffile.imread(file)\n",
    "    img = normalize_img(img)\n",
    "\n",
    "# ...\n",
    "file_list_type_2 = ['x.tif', 'y.tif', 'z.tif']\n",
    "for file in file_list_type_2:\n",
    "    img = tifffile.imread(file)\n",
    "    img = normalize_img(img)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the code in question is only two lines long, I decided to extract it into its own function. Besides the increased readability, I may have noticed in a later stage of my coding that this function isn't as harmless as it seems due to a possible **integer overflow.** (`ZeroDivisionError`) So now the function is longer than two lines, and more tests have to be added. Coding these upgrades in the first case, where we didn't extract the code snippet, would've been double the work with a higher chance for bugs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long functions (with \"block comments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, functions should be about 10-20 lines long in total, including documentation. I.e. a single function or method shouldn't be longer than your screen. There's no lower-bound limit, meaning that very short functions where the code is 2-3 lines long, as seen above, are absolutely fine.\n",
    "\n",
    "Long functions are hard to understand, hard to test and will usually contain several blocks with distinct purposes. There's absolutely no reason to group up blocks that have different \"responsibilities\" in a single function. On occasions in which we _do_ write these long functions, we sometimes like to add block comments to the function, like:\n",
    "\n",
    "```python\n",
    "###################################################\n",
    "# This part deals with reading the data into memory\n",
    "###################################################\n",
    "data = tiffile.imread(...)\n",
    "# ...\n",
    "\n",
    "#############################################\n",
    "# Find the active areas in the processed data\n",
    "#############################################\n",
    "# ...\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Here's a contrived example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(filename):\n",
    "    # Checks for validity of data and reads it\n",
    "    assert pathlib.Path(filename).exists()\n",
    "    raw = tifffile.imread(filename)\n",
    "    assert raw.ndim == 3\n",
    "    assert raw.shape[0] > 1\n",
    "    \n",
    "    # Now we process the data\n",
    "    summed = raw.sum(0)\n",
    "    summed = (summed - summed.mean())\n",
    "    summed /= summed.max()\n",
    "    \n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be clear to you that each of the code blocks, annotated by a comment, should be a different function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objects that should be functions, functions that should be objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"healthy\" code base will contain a mix of objects (with their methods) and functions. Using only one of these programming paradigms in a medium-to-large scale project is probably not the way to go. But how do you decide whether some code has to go into a function, or \"deserves\" it's own object? Here are a couple of thumb rules:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long list of arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever an algorithm has several functions that perform a task back to back, and they all take approximately the same argumets (number of pixels in the image, filename, the data array, etc.) then you should probably turn these functions into methods in an object, and just ditch the arguments by using `self`.\n",
    "\n",
    "This refactoring into an object will also let you organize the code and improve its readability. As separate functions you might not remember who do you call first - do I first `divide_by_largest()` and then `find_most_popular()` or the other way around? As methods in an object you can sort them in a `main()`, publically-available method which exposes the only true way to use these functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objects with either one or two methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually if you have an object which has no more than a couple of methods, it's best to just turn these methods into functions and use them instead. Objects create more boilerplate and clutter, and testing will be generally harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard to explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one might sound naive or simplistic, but it's profoundly true. When things are engineered correctly, they are easier to explain. Try to spend time considering the purpose of each distinguishable building block of your code, how it interacts with the other pieces, what information it requires, what other parts of your code make use of this information, how else are they related, etc. With enough practice, you'll find that the way you write code in the first place changes and requires less revisions and rewrites every time something in your workflow changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Too many nested levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having too many nested levels in your code gives the readers of it a harder time - they have to remember the last condition that was met (or wasn't), and to understand its relation to the current condition. But how do we do that? We have two main methods: early returns and \"switch-like\" statements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BAD CODE BELOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a, b, c, d):\n",
    "    if a > b:\n",
    "        c = func1(a)\n",
    "        if c:\n",
    "            print(f\"C is {c}\")\n",
    "            for item in c:\n",
    "                d = [m for m in item if item is not None]\n",
    "        else:\n",
    "            return None\n",
    "        return d\n",
    "    else:\n",
    "        c = func1(b)\n",
    "        if c:\n",
    "            print(f\"C is {c}\")\n",
    "            d = []\n",
    "            for item in c:\n",
    "                d.append([m for m in item if item is not None])\n",
    "        else:\n",
    "            return None\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be better to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a, b, c, d):\n",
    "    c = func1(max(a, b))\n",
    "    if not c:\n",
    "        return None\n",
    "\n",
    "    print(f\"C is {c}\")\n",
    "    d = []\n",
    "    for item in c:\n",
    "        d.append([m for m in item if item is not None])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main differences here:\n",
    "* The use of the built-in `max()` function to drop the first `if` statements, since the two code paths are identical.\n",
    "* \"Early\" `return`. Instead of asking `if c:` and then having a fully-indented code block below, we reverse the condition, asking `if not c: return None`, and then we can safely unindent the following code path, since we're sure that `c` has the right value for us. It's also easier to read, since you can remember that for all lines of code below the `if not c` condition, `c` is not `False` or `None`, there are no `else` clauses that would make it less obvious was condition are we really checking at this line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Switch-like statement in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Progammers in other languages, including MATLAB, are usually aware of the `switch - case` operator which allows you to choose what to do based on a specific value of some variable during runtime. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func1():\n",
    "    return 4\n",
    "\n",
    "def my_func2(data):\n",
    "    print(f\"Data is {data}\")\n",
    "\n",
    "def my_func3(data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{code-block}\n",
    "data = my_func1()\n",
    "switch data:\n",
    "    case 4:\n",
    "        my_func2(data)\n",
    "    case 15:\n",
    "        my_func3(data)\n",
    "    # etc...\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strike>Python doesn't have a proper switch statement</strike>, but you can mimick this behavior using dictionaries! Here's an equivalent piece of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is 4\n"
     ]
    }
   ],
   "source": [
    "switch = {4: my_func2, 15: my_func3}\n",
    "data = my_func1()\n",
    "switch[data](data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we access the `switch` dictionary at the index `data`, we get back the name of the function which was mapped there. This is like running the following statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = my_func3\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `a` is just a reference to the function. Printing it doesn't call the function, we have to add parenthesis in order for the function to be executed. And this is why we have the `(data)` part after `switch[data]` - the parenthesis, with the argument inside them, make the actual function call happen.\n",
    "\n",
    "Switch statements aren't too common out in the wild, but sometimes they fit best your mental model of your code. When that is the case, a dictionary is a suitable replacement for the missing `switch`. By the way, there are libraries which try to mimick a `switch` in a clearer manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{note}\n",
    "Python 3.10 introduced switch statements, so if you *really* don't want to use a dictionary go ahead and:\n",
    "```{code-block}\n",
    "def f(x):\n",
    "    match x:\n",
    "        case 'a':\n",
    "            return 1\n",
    "        case 'b':\n",
    "            return 2\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Design Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous part dealt with low-level concepts with very clear \"do's and don'ts\". We'll now turn our heads to some higher-level concepts when you think of the design of your software. Most of the ideas presented below are from Robert Martin's, AKA Uncle Bob, lectures and textbooks. He's one of the founding fathers of object-oriented design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Orthogonality and Encapsulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases objects interact with one another. In the case of some `ProcessData` class, which might process some instances of a `Data` class, that can contain a couple of `Series` and metadata, for example, we can see how `ProcessData` communicates with the data inside the `Data` class, modifying it further. \n",
    "\n",
    "A preliminary design might look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Data:\n",
    "    \"\"\" Simple container for DataFrames and their metadata \"\"\"\n",
    "    def __init__(self, arr1: np.ndarray, arr2: np.ndarray, date: float):\n",
    "            self.ser1 = pd.Series(arr1, dtype=np.uint8)\n",
    "            self.ser2 = pd.Series(arr2, dtype=np.int16)\n",
    "            self.metadata = dict(shape1=self.ser1.shape,\n",
    "                                 shape2=self.ser2.shape,\n",
    "                                 total=self.ser1.shape[0] + self.ser2.shape[0],\n",
    "                                 date=date)\n",
    "\n",
    "            \n",
    "class ProcessData:\n",
    "    \"\"\" Pipeline to process twin Data instances \"\"\"\n",
    "    def __init__(self, data1: Data, data2: Data):\n",
    "        self.data1 = data1\n",
    "        self.data2 = data2\n",
    "        self.result = []\n",
    "        self.metadata = dict(columns1=data1.columns,\n",
    "                             columns2=data2.columns,\n",
    "                             metadata=data1.metadata)\n",
    "        \n",
    "    def process(self):\n",
    "        self.result.extend([data1.ser1.sum(), data2.ser1.sum()])\n",
    "        self.result.append([data1.ser1.mean() + data2.ser2.mean()])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have here a `Data` class which serves as a container for two DataFrames that are logically connected. It also simplifies the access to some of the metadata contained with theses DataFrames.\n",
    "\n",
    "We also have a `ProcessData` class that uses the `Data` instances to calculate some statistical properties and keep them for later use.\n",
    "\n",
    "While this design works (which is important), it's flawed in the sense that the `ProcessData` object is highly dependent on the implementation details of the `Data` class. How would you write tests for `ProcessData`? Many of the possible tests you may write are reliant on proper `Data` implementation. When higher-level objects are dependent on specific attributes of some lower-level module, we need to perform [\"dependency inversion\"](https://en.wikipedia.org/wiki/Dependency_inversion_principle). This decoupling process can also be called \"object orthogonality\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do a couple of major changes to our design which will solve, step by step, the design issues we encoutered.\n",
    "\n",
    "First we'll create a new `DataContainer` class that holds `Data` instances, and redefine the `Data` class more appropriately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    \"\"\" Simple container for DataFrames and their metadata \"\"\"\n",
    "    def __init__(self, arr1: np.ndarray, arr2: np.ndarray, date: float):\n",
    "            self._ser1 = pd.Series(arr1, dtype=np.uint8)\n",
    "            self._ser2 = pd.Series(arr2, dtype=np.int16)\n",
    "            self._metadata = dict(shape1=self.df1.shape,\n",
    "                                 shape2=self.df2.shape,\n",
    "                                 total=self.df1.shape[0] + self.df2.shape[0],\n",
    "                                 date=date)\n",
    "        \n",
    "    @property\n",
    "    def data(self):\n",
    "        \"\"\" Returns the actual data variables as an iterable\"\"\"\n",
    "        result = [self._ser1, self._ser2]\n",
    "        return result\n",
    "    \n",
    "    @property\n",
    "    def metadata(self):\n",
    "        return self._metadata\n",
    "    \n",
    "    def sum(self):\n",
    "        return [x.sum() for x in self.data]\n",
    "\n",
    "\n",
    "class DataContainer:\n",
    "    \"\"\" Holds, in order, instances of Data \"\"\"\n",
    "    def __init__(self, datas):\n",
    "        self._data = []\n",
    "        self._metadata = {}\n",
    "        try:\n",
    "            for idx, data in enumerate(datas):\n",
    "                if isinstance(data, Data):\n",
    "                    self._data.append(data)\n",
    "                    self._metadata[idx] = data.metadata\n",
    "                else:\n",
    "                    raise TypeError(f\"TypeError: Data {data} isn't a 'Data' type.\")\n",
    "        except TypeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "    \n",
    "    @property\n",
    "    def metadata(self):\n",
    "        return self._metadata\n",
    "    \n",
    "    def sum(self):\n",
    "        result = []\n",
    "        for data in self._data:\n",
    "            result.append(data.sum())\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First note the \"new technical term\": We introduce here the `@property` decorators. If we define some method as a property, that keyword can be used like a regular attribute, except for the fact that it's immutable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original attribute: 2\n",
      "Attributes can be changed: 3\n",
      "------\n",
      "Using the method: 2\n",
      "And of course, it can't be changed (though you could override the function).\n",
      "------\n",
      "As a property: 2\n",
      "AttributeError: can't set attribute - properties can't (by default) be changed.\n"
     ]
    }
   ],
   "source": [
    "class Trial:\n",
    "    def __init__(self):\n",
    "        self.two_as_attr = 2\n",
    "    \n",
    "    def two_as_method(self):\n",
    "        return 2\n",
    "    \n",
    "    @property\n",
    "    def two_as_prop(self):\n",
    "        return 2\n",
    "\n",
    "tr = Trial()\n",
    "\n",
    "# Changing attributes is possible:\n",
    "print(f\"The original attribute: {tr.two_as_attr}\")\n",
    "tr.two_as_attr = 3\n",
    "print(f\"Attributes can be changed: {tr.two_as_attr}\")\n",
    "print(\"------\")\n",
    "\n",
    "# Using the regular method requires brackets\n",
    "print(f\"Using the method: {tr.two_as_method()}\")\n",
    "print(\"And of course, it can't be changed (though you could override the function).\")\n",
    "print(\"------\")\n",
    "\n",
    "# Using a property \"feels\" like using an attributes:\n",
    "print(f\"As a property: {tr.two_as_prop}\")  # no brackets\n",
    "try:\n",
    "    tr.two_as_prop = 3  # AttributeError\n",
    "except AttributeError as e:\n",
    "    print(f\"AttributeError: {e} - properties can't (by default) be changed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But besides this new, exciting feature of Python, what else has changed with the implementation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Data`:\n",
    "1. We redefined `Data`. The new object doesn't allow anyone from the outside to change the data it holds, it only allows for a \"view\" of the data. The use of properties ensure that once the object was created, the internal structure of the instance remains intact. The single underscore before the variable names also prevents direct access to the attribute. This idea is called _encapsulation_.\n",
    "\n",
    "2. Furthermore, if we examine the `sum()` method, we see that it's now bound to the `Data` object itself. If we write it explicitly it makes senes: _The sum of the data is a bound method to our data - an intrinsic property of it._ If we ever decide to change how our data is stored, the `sum()` method should change accordingly, but no other object will be affected.\n",
    "\n",
    "\n",
    "#### `DataContainer`:\n",
    "1. The new `DataContainer` class _doesn't really know_ what it's holding. All it cares is that they're `Data` instances. It doesn't peek inside the methods of the different `Data` instances.\n",
    "\n",
    "2. It doesn't allow access to the list of `Data` instances itself. It exposes a `data` property which returns the list. If we decide to change the internal implementation of `DataContainer`, users of this class wouldn't care as long as we keep the output of the `data` property similar. Even if the list is empty, it will always return something.\n",
    "\n",
    "Let's see the redefined implementation of the `ProcessData` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessData:\n",
    "    \"\"\" Pipeline to process twin Data instances \"\"\"\n",
    "    def __init__(self, data_container: DataContainer):\n",
    "        self.data_container = data_container\n",
    "        self.result = {}\n",
    "        self.metadata = data_container.metadata\n",
    "        \n",
    "    def process(self):\n",
    "        \"\"\" Mock processing pipeline \"\"\"\n",
    "        self.result['sum'] = self.data_container.sum()\n",
    "        means = [x.mean() for x in self.data_container.data]\n",
    "        self.result['mean'] = means\n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet above is now much cleaner than the one we had beforehand. It uses the \"API\" of the `DataContainer` in two ways; either using a fully-featured `sum()` function, or by (securely) accessing the data using the `data` property and running non-standard processing on it (mean calculation in our case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downside is the added class - more code to write, more tests, more imports at the top. But the added value is tremendous. Think how easy it is to add new functionality into the pipeline. Everything is flexible, allowing to create a new `median()` function in the `DataContainer` class, for example. We can even change the internal structure of the `Data` class and still use the downstream class effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes as Data Types and Class Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another fairly important use case for classes in Python is their as user-defined types for particular data. Programming languages define for us the basic types of data: floating-point numbers, integres, string and so on. But what if (some of) our data is not composed of these primitive types? Can we construct data types of our own?\n",
    "\n",
    "For instane, assume I'm collecting data from participants in a study I'm running, and one of the data points I'm gatheting is their age. How should I encode it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The age of a person is not an integer number. It _can_ be thought of as a floating point number, but then being 41.9 means that your age is 41 years and almost eleven months, which isn't too obvious from just looking at 41.9, since the 9 could be interpreter as the month of September. We can try to write stuff like '41.9' or '41 years and 9 months' or '41.9.14' but it doesn't look so good.\n",
    "\n",
    "**Instead,** what we should do is to write a class that defines an age:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "class Grade:\n",
    "    \"\"\"\n",
    "    Represenets a single grade.\n",
    "    \"\"\"\n",
    "    def __init__(self, value: Union[int, float]):\n",
    "        self._value = self._verify_grade(value)\n",
    "\n",
    "    def _verify_grade(self, value: Union[int, float]):\n",
    "        \"\"\"Verifies that the given grade holds up to our standards\"\"\"\n",
    "        if value < 0:\n",
    "            raise ValueError('too low')\n",
    "        if value > 100:\n",
    "            raise ValueError('too high')\n",
    "        return value\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self._value\n",
    "\n",
    "    @value.setter\n",
    "    def value(self, value):\n",
    "        self._value = self._verify_grade(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "YAERS_OUT_OF_RANGE = \"Years should be a valid integer, received {years}\"\n",
    "MONTHS_OUT_OF_RANGE = \"Months should be an integer between 1 and 12, received {months}\"\n",
    "DAYS_OUT_OF_RANGE = \"Days should be an integer between 1 and 31, received {days}\"\n",
    "\n",
    "class Age:\n",
    "    \"\"\"\n",
    "    Represents the age of a person.\n",
    "    \"\"\"\n",
    "    def __init__(self, years: int, months: int = 1, days: int = 1):\n",
    "        self.validate_input(years, months, days)\n",
    "        self.years = years\n",
    "        self.months = months\n",
    "        self.days = days\n",
    "    \n",
    "    def validate_input(self, years: int, months: int, days: int):\n",
    "        valid_years = isinstance(years, int) and 0 <= years < 150\n",
    "        valid_months = isinstance(months, int) and 0 < months < 13\n",
    "        valid_days = isinstance(days, int) and 0 < days < 32\n",
    "        if not valid_years:\n",
    "            message = OUT_OF_RANGE.format(years=years)\n",
    "            raise TypeError(message)\n",
    "        if not valid_months:\n",
    "            message = MONTHS_OUT_OF_RANGE.format(months=months)\n",
    "            raise TypeError(message)\n",
    "        if not valid_days:\n",
    "            message = DAYS_OUT_OF_RANGE.format(days=days)\n",
    "            raise TypeError(message)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the DataFrame or array containing our data can have a column of type Age which will contain meaningful data about the persons age. Notice how compact this class is. It doesn't contain the ID number of the person, nor it's name. All it does is encode the age. It's important that each of the class we write will have one specific purpose, and not more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we're not quite done here. There is another possible \"representation\" of age and that is the date of birth. It's quite a natural requirement that given a date of birth, a string or a datetime object, our `Age` class will know how to generate a proper `Age` instance. Similarly, given an `Age` instance, we should be able to generate the person's date of birth. \n",
    "\n",
    "The second requirement is pretty easy, we can simply make a `get_dob()` method that returns the date of birth. But how should we approach the first requirement, of instantiating an `Age` from a given date? Let's try to _refactor_ our class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "class Age:\n",
    "    \"\"\"\n",
    "    Represents the age of a person.\n",
    "    \"\"\"\n",
    "    \n",
    "    cur_year = datetime.date.today().year\n",
    "    \n",
    "    @classmethod\n",
    "    def from_str(cls, date_str):\n",
    "        \"\"\" Instantiate from a string containing a date in the standard ISO format. \"\"\"\n",
    "        try:\n",
    "            date = datetime.date.fromisoformat(date_str)\n",
    "        except ValueError:\n",
    "            raise\n",
    "        else:    \n",
    "            return cls(cls.cur_year - date.year, date.month, date.day)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dob(cls, dob):\n",
    "        \"\"\" Instatiates from a datetime.date or a datetime.datetime object \"\"\"\n",
    "        try:\n",
    "            years = dob.year\n",
    "            months = dob.month\n",
    "            days = dob.day\n",
    "        except AttributeError:\n",
    "            print(f\"Input should be a datetime.datetime or a datetime.date instance. Received {dob} which is a {type(dob)}.\")\n",
    "            raise\n",
    "        else:\n",
    "            return cls(years, months, days)    \n",
    "    \n",
    "    def __init__(self, years: int, months: int = 1, days: int = 1):\n",
    "        self.validate_input(years, months, days)\n",
    "        self.years = years\n",
    "        self.months = months\n",
    "        self.days = days\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"Age(years={self.years}, months={self.months}, days={self.days})\"\n",
    "\n",
    "    def validate_input(self, years: int, months: int, days: int):\n",
    "        valid_years = isinstance(years, int) and 0 <= years < 150\n",
    "        valid_months = isinstance(months, int) and 0 < months < 13\n",
    "        valid_days = isinstance(days, int) and 0 < days < 32\n",
    "        if not valid_years:\n",
    "            message = OUT_OF_RANGE.format(years=years)\n",
    "            raise TypeError(message)\n",
    "        if not valid_months:\n",
    "            message = MONTHS_OUT_OF_RANGE.format(months=months)\n",
    "            raise TypeError(message)\n",
    "        if not valid_days:\n",
    "            message = DAYS_OUT_OF_RANGE.format(days=days)\n",
    "            raise TypeError(message)       \n",
    "        \n",
    "    def get_dob(self):\n",
    "        \"\"\" Returns the date of birth \"\"\"\n",
    "        return datetime.date(self.cur_year - self.years, self.months, self.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age(years=20, months=4, days=5)\n"
     ]
    }
   ],
   "source": [
    "age = Age(42, 11, 1)\n",
    "age.get_dob()\n",
    "age2 = Age.from_str('2001-04-05')\n",
    "print(age2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typestates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typestates are a way to enforce the state of our data\\application with strict types.\n",
    "\n",
    "\n",
    "Let's assume I have 24 human volunteers in a combined fMRI + questionnaire study. I keep them all in a single DataFrame for brevity and ease-of-use, but in effect they're in different stages of my experiment. A few were just recruited last week, and I haven't even set a date for our first meeting. A few others were already scanned in the magnet once, but still have to go through my second questionnaire session. \n",
    "\n",
    "My application monitors these students, alerts me of incoming meeting dates, and (of course) analyzes the results of the questionnaires and scans.\n",
    "\n",
    "The __correctness__ of this application can be enforced in many ways - tests, mock data, daily use - but here I choose to show another mechanism - typestates. The fact that the current status of each volunteer isn't specified with a simple string in a table, but is actually a different class altogether, is another way to make sure that I always receive the expected output from each method call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Helper types\n",
    "class Name:\n",
    "    \"\"\" First and last name \"\"\"\n",
    "    # Implementation omitted\n",
    "\n",
    "\n",
    "class Age:\n",
    "    \"\"\" Special age type \"\"\"\n",
    "    # Implementation omitted\n",
    "\n",
    "\n",
    "class FmriResult:\n",
    "    \"\"\" Results from an fMRI scan \"\"\"\n",
    "    # Implementation omitted\n",
    "\n",
    "\n",
    "# Volunteer types    \n",
    "class Volunteer:\n",
    "    \"\"\" Base class for all volunteers in my project \"\"\"\n",
    "    def __init__(self, name: Name, age: Age, call_date: datetime.time, vol_id: int):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.call_date = call_date\n",
    "        self.id = vol_id\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self.name}, age {self.age}, first called at {self.call_date}.\"\n",
    "        \n",
    "    def update_df(self, records: pd.DataFrame):\n",
    "        \"\"\" Add the instance to the dataframe containing the rest of the data \"\"\"\n",
    "        record = pd.DataFrame([self.name, self.age, self.call_date, \n",
    "                               self.id, self.metadata, type(self), copy.copy(self)])\n",
    "        records.append(record)\n",
    "        return records\n",
    "    \n",
    "    def remove_from_df(self, records: pd.DataFrame):\n",
    "        \"\"\" Remove the instance from the student records \"\"\"\n",
    "        idx = records.id == self.id\n",
    "        records.drop(idx, inplace=True)\n",
    "        return records\n",
    "\n",
    "    \n",
    "class PreScanOne(Volunteer):\n",
    "    \"\"\" Volunteer before the first session \"\"\"\n",
    "    loc = 0  # ordinal place in hierarchy\n",
    "    \n",
    "    def __init__(self, name: Name, age: Age, call_date: datetime.time, vol_id: int, \n",
    "                 scan_one_date: datetime.time):\n",
    "        super().__init__(name, age, call_date, vol_id)\n",
    "        self.metadata = dict(scan_one_date=scan_one_date)\n",
    "        \n",
    "    def advance(self, result: FmriResult, next_date: datetime.time):\n",
    "        \"\"\" Advance a PreScanOne to a PostScanOne \"\"\"\n",
    "        new = PostScanOne(self, result, next_date)\n",
    "        return new\n",
    "    \n",
    "\n",
    "class PostScanOne(Volunteer):\n",
    "    \"\"\" Volunteer after the first session \"\"\"\n",
    "    loc = 1\n",
    "    \n",
    "    def __init__(self, pre_volunteer: PreScanOne, scan_one_data: FmriResult, \n",
    "                 scan_two_date: datetime.time):\n",
    "        super().__init__(pre_volunteer.name, pre_volunteer.age, pre_volunteer.call_date, pre_volunteer.id)\n",
    "        self.metadata = pre_volunteer.metadata\n",
    "        self.metadata['scan_one_data'] = scan_one_data\n",
    "        self.metadata['scan_two_date'] = scan_two_date\n",
    "    \n",
    "    def advance(self, result: FmriResult, next_date: datetime.time):\n",
    "        \"\"\" Advance a PostScanOne to a PreScanTwo \"\"\"\n",
    "        new = PreScanTwo(self, result, next_date)\n",
    "        return new\n",
    "\n",
    "\n",
    "# Examples of generic methods that use this interface\n",
    "def advance_volunteer(old_vol, results: FmriResult, records: pd.DataFrame):\n",
    "    \"\"\" \n",
    "    Move volunteer to next step in the experiment, returning the new \n",
    "    instance and records.\n",
    "    \"\"\"\n",
    "    old_vol.remove_from_df(records)\n",
    "    new_vol = old_vol.advance(results, records)\n",
    "    new_vol.update_df(records)\n",
    "    return new_vol, records\n",
    "\n",
    "\n",
    "def process_data(records):\n",
    "    \"\"\" Run the same processing function over all fMRI data \"\"\"\n",
    "    results = []\n",
    "    for vol in records:\n",
    "        try:\n",
    "            results.append(vol.process_data())\n",
    "        except AttributeError:  # instance doesn't have data\n",
    "            pass\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is long, but interesting, so let's try to break it down.\n",
    "\n",
    "At the beginning we have a few help classes which I merely defined, but not implemented. These shouldn't look strange to you. We talked during class of how an `Age` type is an important example of defining our own types in a program, since it's neither an integer nor a floating point number.\n",
    "\n",
    "The second part is the most interesting. We have a base class called `Volunteer` which contains basic information which is common to all experiment volunteers. But it's actually more than that - it also defines the _interfaces_ between the classes, it forces the classes to have specific attributes that will comply to this protocol, linking their behavior together.\n",
    "\n",
    "The other two classes inherit from `Volunteer` and represent the first two steps in the \"Volunteer path\". The `loc` class variable signifies that. From phase one (`PreScanOne`( a volunteer can only advance forward (or drop out from the experiment) to step 2. And likewise from step 2 to 3 - you'll always find the same `.advance()` method that takes you to the next step, even though the implementation is slightly different. To handle the variability in the held data, we have the `metadata` attribute which can hold different parameters and datapoints.\n",
    "\n",
    "The last part shows how to use such an interface. We have a function that advances an instance of a class \"one step\" to the next phase. We have a function that runs some processing on the data held inside the instances, and we can have as many functions (and classes as we wish). It's completely extensible since the API is well-defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Concepts and Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, good and clear software design can be aided from using unique Python features and packages. We'll review a few of the more prominent ones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code formatting, styling and linting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you remember, from day one I insisted that your code should have a very specific look, as defined in PEP8, the official document describing how to style your Python. Happily enough there are a few tools that can automate this work for us, and the most famous one is `black`, which can be operated from either the command line or directly from VSCode. I'll show how to do it in after we review a few other libraries of the same type right below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Annotations and MyPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since version 3.6, Python allows this syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "\n",
    "def doer_of_stuffs(a: float, b: int, c: str = 'ccc') -> Tuple[str, Dict[int, float]]:\n",
    "    \"\"\"\n",
    "    Does stuff to a, b, and c.\n",
    "    Returns: A tuple of a string and a dictionary mapping ints to floats\n",
    "    \"\"\"\n",
    "    a_helper: float = a + 2\n",
    "    b_helper: float = b / 3\n",
    "    int_a = int(a_helper)\n",
    "    c2: str = c + c\n",
    "    return c2, {b: a_helper, int_a: b_helper}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a bit more verbose, these _type annotations_ make things clearer when dealing with large codebases. Knowing the defined type of your variables as they bounce around between modules and functions can help with the debugging process of your code tremendously.\n",
    "\n",
    "Moreover, modern IDEs like PyCharm and VSCode will alert you before you run the code of any possible type errors. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    a = 3  # integer\n",
    "    a /= 2  # now it's a float\n",
    "    arr = np.array([1, 2, 3])\n",
    "    \n",
    "    # ... lots of code here\n",
    "    \n",
    "    b = arr[a]  # TypeError - cannot index with a float variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VSCode will mark this `arr[a]` expression and try to prevent you from running this code. \n",
    "\n",
    "A more wholesome approach is [`mypy`](https://github.com/python/mypy), which was developed in Dropbox, a company very reliant on its Python-based product. When the Dropbox codebase increased in size, its engineers wanted to keep using Python due to its amazing features, but avoid the problems that come with a dynamically-typed language. Thus, `mypy` was born. In essence, it's a command-line tool that runs type checks on the entirety of your code base, verifying the type-correctness of your application. In many places a clean `mypy` error log is required before committing changes to the code base.\n",
    "\n",
    "`mypy` supports both comment-based type annotations for older versions of Python (Dropbox used Python 2.7 until 2019) and the new style of type annotations shown above. It can also generate type annotations on the fly, using `PyAnnotate`, while you run your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linters (`pylint`, `flake8`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last tools we'll dicuss are linters, which check the correctness of your code in several key aspects. First, they point out violations of simple PEP8 rules, like wrong variable naming and such. But more importantly they'll make sure that your code is in a runnable state, which means that you're not using variables you haven't declared, or libraries which you haven't imported, and such. Just like `black` and `mypy`, these two tools can also be configured to work with VSCode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enumerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python added enumeration support in Python 3.4, and it's starting to pop-up more and more in new code bases. An enumeration is a list of discrete possible values. Assuming I have a simple addition function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_or_sub(a, b, add=True):\n",
    "    \"\"\" Simple addition\\subtraction \"\"\"\n",
    "    return a + b if add else a - b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of possible values for `a` and `b` is endless, so these cannot be enumerated. The `add` keyword is called a \"flag\", since it has two possible values - `True` and `False`. It's an enumeration of two possible values.\n",
    "\n",
    "When we have more than two options, or when our two options aren't simply booleans, we can use an enumeration. Here's a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Color(Enum):\n",
    "    RED = 'r'\n",
    "    GREEN = 'g'\n",
    "    BLUE = 'b'\n",
    "    BLACK = 'k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the `Enum`'s attributes now has a `name` and a `value`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RED\n",
      "r\n"
     ]
    }
   ],
   "source": [
    "print(Color.RED.name)\n",
    "print(Color.RED.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"real world\" enumerations aren't too popular due to the fact that they were introduced very late. But a use-case could look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n",
       "               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08',\n",
       "               '2018-01-09', '2018-01-10', '2018-01-11', '2018-01-12',\n",
       "               '2018-01-13', '2018-01-14', '2018-01-15', '2018-01-16',\n",
       "               '2018-01-17', '2018-01-18', '2018-01-19', '2018-01-20',\n",
       "               '2018-01-21', '2018-01-22', '2018-01-23', '2018-01-24',\n",
       "               '2018-01-25', '2018-01-26', '2018-01-27', '2018-01-28',\n",
       "               '2018-01-29', '2018-01-30', '2018-01-31', '2018-02-01',\n",
       "               '2018-02-02', '2018-02-03', '2018-02-04', '2018-02-05',\n",
       "               '2018-02-06', '2018-02-07', '2018-02-08', '2018-02-09',\n",
       "               '2018-02-10', '2018-02-11', '2018-02-12', '2018-02-13',\n",
       "               '2018-02-14', '2018-02-15', '2018-02-16', '2018-02-17',\n",
       "               '2018-02-18', '2018-02-19', '2018-02-20', '2018-02-21',\n",
       "               '2018-02-22', '2018-02-23', '2018-02-24', '2018-02-25',\n",
       "               '2018-02-26', '2018-02-27', '2018-02-28', '2018-03-01',\n",
       "               '2018-03-02', '2018-03-03', '2018-03-04', '2018-03-05',\n",
       "               '2018-03-06', '2018-03-07', '2018-03-08', '2018-03-09',\n",
       "               '2018-03-10', '2018-03-11', '2018-03-12', '2018-03-13',\n",
       "               '2018-03-14', '2018-03-15', '2018-03-16', '2018-03-17',\n",
       "               '2018-03-18', '2018-03-19', '2018-03-20', '2018-03-21',\n",
       "               '2018-03-22', '2018-03-23', '2018-03-24', '2018-03-25',\n",
       "               '2018-03-26', '2018-03-27', '2018-03-28', '2018-03-29',\n",
       "               '2018-03-30', '2018-03-31', '2018-04-01', '2018-04-02',\n",
       "               '2018-04-03', '2018-04-04', '2018-04-05', '2018-04-06',\n",
       "               '2018-04-07', '2018-04-08', '2018-04-09', '2018-04-10'],\n",
       "              dtype='datetime64[ns]', freq='D')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "rng = pd.date_range('1/1/2018',periods=100, freq='D')  # 'D' is days\n",
    "rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31', '2018-04-30',\n",
       "               '2018-05-31', '2018-06-30', '2018-07-31', '2018-08-31',\n",
       "               '2018-09-30', '2018-10-31', '2018-11-30', '2018-12-31',\n",
       "               '2019-01-31', '2019-02-28', '2019-03-31', '2019-04-30',\n",
       "               '2019-05-31', '2019-06-30', '2019-07-31', '2019-08-31',\n",
       "               '2019-09-30', '2019-10-31', '2019-11-30', '2019-12-31',\n",
       "               '2020-01-31', '2020-02-29', '2020-03-31', '2020-04-30',\n",
       "               '2020-05-31', '2020-06-30', '2020-07-31', '2020-08-31',\n",
       "               '2020-09-30', '2020-10-31', '2020-11-30', '2020-12-31',\n",
       "               '2021-01-31', '2021-02-28', '2021-03-31', '2021-04-30',\n",
       "               '2021-05-31', '2021-06-30', '2021-07-31', '2021-08-31',\n",
       "               '2021-09-30', '2021-10-31', '2021-11-30', '2021-12-31',\n",
       "               '2022-01-31', '2022-02-28', '2022-03-31', '2022-04-30',\n",
       "               '2022-05-31', '2022-06-30', '2022-07-31', '2022-08-31',\n",
       "               '2022-09-30', '2022-10-31', '2022-11-30', '2022-12-31',\n",
       "               '2023-01-31', '2023-02-28', '2023-03-31', '2023-04-30',\n",
       "               '2023-05-31', '2023-06-30', '2023-07-31', '2023-08-31',\n",
       "               '2023-09-30', '2023-10-31', '2023-11-30', '2023-12-31',\n",
       "               '2024-01-31', '2024-02-29', '2024-03-31', '2024-04-30',\n",
       "               '2024-05-31', '2024-06-30', '2024-07-31', '2024-08-31',\n",
       "               '2024-09-30', '2024-10-31', '2024-11-30', '2024-12-31',\n",
       "               '2025-01-31', '2025-02-28', '2025-03-31', '2025-04-30',\n",
       "               '2025-05-31', '2025-06-30', '2025-07-31', '2025-08-31',\n",
       "               '2025-09-30', '2025-10-31', '2025-11-30', '2025-12-31',\n",
       "               '2026-01-31', '2026-02-28', '2026-03-31', '2026-04-30'],\n",
       "              dtype='datetime64[ns]', freq='M')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = pd.date_range('1/1/2018',periods=100, freq='M')  # it can also be 'M'\n",
    "rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the possible values for the `freq` keyword? Day is `D`, month is `M`, Year will probably be `Y`. Are there any more keywords? Will `d` also work, or do I have to use capital `D`? Actually, checking the [official](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.date_range.html) documentation doesn't result in anything too useful.\n",
    "\n",
    "This is where enumerations come into play. This could've been simpler if we could only choose a value from a list of possible values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'DateRangeFreq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-4b01d1e87139>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'years'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1/1/2018'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mperiods\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDateRangeFreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# doesn't actually work...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/python_for_neuroscientists/textbook/venv/lib/python3.8/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_SparseArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module 'pandas' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'DateRangeFreq'"
     ]
    }
   ],
   "source": [
    "class DateRangeFreq(Enum):\n",
    "    D = 'days'\n",
    "    M = 'months'\n",
    "    Y = 'years'\n",
    "\n",
    "rng = pd.date_range('1/1/2018',periods=100, freq=pd.DateRangeFreq.D)  # doesn't actually work..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were unsure of the available parameters, we could import the `DateRangeFreq` object and inspect its possible values. As you can see, each key has a value associated with it. This value can be an integer, string or even a Python object.\n",
    "\n",
    "Enumerations are gaining popularity in the Python ecosystem and are a simple thing that's great to implement and increase the robustness of your codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `attrs` - Classes without boilerplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python classes are extremely useful, but they're also pretty verbose. They require you to write a lot of code for very basic operations.\n",
    "\n",
    "For example, in the the `__init__()` method you have to go through each variable in the function signature and assign it to your own value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example:\n",
    "    def __init__(self, param1, param2, param3='no', param4):\n",
    "        self.param1 = param1\n",
    "        self.param2 = param2\n",
    "        self.param3 = param3\n",
    "        self.param4 = param4\n",
    "    \n",
    "    def my_method(self):\n",
    "        \"\"\" Do stuff \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So many lines of repetitive code doing basically nothing. I didn't assert the types of the variables, I didn't do some basic pre-processing - this is called \"boilerplate\" code. Python requires me to write these tedious lines every time I create a class, and when classes get bigger and bigger, these assignments can be a hassle to write."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`attrs` to the rescue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import attr\n",
    "from attr.validators import instance_of\n",
    "\n",
    "\n",
    "@attr.s\n",
    "class ExampleTwo:\n",
    "    param1 = attr.ib(validator=instance_of(int))\n",
    "    param2 = attr.ib(validator=instance_of(float))\n",
    "    param3 = attr.ib(default='no')\n",
    "    param4 = attr.ib(default=attr.Factory(list))\n",
    "    \n",
    "    def my_method(self):\n",
    "        \"\"\" Do stuff \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "@attr.s(auto_attribs=True)\n",
    "class ExampleThree:\n",
    "    param1: int\n",
    "    param2: float\n",
    "    param3 = 'no'\n",
    "    param4 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExampleTwo(param1=1, param2=2.0, param3='a', param4=[4, 5, 6])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ExampleTwo(1, 2., 'a', [4, 5, 6])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "(\"'param1' must be <class 'int'> (got 1.1 that is a <class 'float'>).\", Attribute(name='param1', default=NOTHING, validator=<instance_of validator for type <class 'int'>>, repr=True, eq=True, order=True, hash=None, init=True, metadata=mappingproxy({}), type=None, converter=None, kw_only=False, inherited=False, on_setattr=None), <class 'int'>, 1.1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-b26121f2f7cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExampleTwo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<attrs generated init __main__.ExampleTwo-2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, param1, param2, param3, param4)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__attr_factory_param4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_validators\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0m__attr_validator_param1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__attr_param1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0m__attr_validator_param2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__attr_param2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/python_for_neuroscientists/textbook/venv/lib/python3.8/site-packages/attr/validators.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inst, attr, value)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \"\"\"\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0;34m\"'{name}' must be {type!r} (got {value!r} that is a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \"{actual!r}).\".format(\n",
      "\u001b[0;31mTypeError\u001b[0m: (\"'param1' must be <class 'int'> (got 1.1 that is a <class 'float'>).\", Attribute(name='param1', default=NOTHING, validator=<instance_of validator for type <class 'int'>>, repr=True, eq=True, order=True, hash=None, init=True, metadata=mappingproxy({}), type=None, converter=None, kw_only=False, inherited=False, on_setattr=None), <class 'int'>, 1.1)"
     ]
    }
   ],
   "source": [
    "b = ExampleTwo(1.1, 2., 'a', [4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. No `__init__` is required, each `paramX` variable is already assigned to `self.paramX`. It also allows the addition of validators, default values, converter functions (not shown), and it even implements the comparison methods (`__eq__`, `__gt__`, etc.) for you. It has a ton of other useful features which I won't go into right now, but you can be sure that it's a package worth using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality analysis and units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with numbers that have units, it's usually a good idea to keep the physical quantity assigned to that value as close as possible.\n",
    "\n",
    "When you're measuring the local field potential using some electrode array, it's good practice to verify that throughout the entirety of your processing pipeline, the voltage values aren't divided by a number with units of time, because units of _[Volts] / [seconds]_ usually have no physical meaning. It can also help you assert that your dF/F calculation indeed has natural units, and not some other arbitrary units.\n",
    "\n",
    "There are many options in the Python world for dimensionality analysis. If you're using Python to write symbolic math and solve equations, I suggest you use SymPy's `physics.units` module. Else - use `pint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "3.04 meter"
      ],
      "text/latex": [
       "$3.04\\ \\mathrm{meter}$"
      ],
      "text/plain": [
       "3.04 <Unit('meter')>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pint\n",
    "\n",
    "\n",
    "ureg = pint.UnitRegistry()\n",
    "3 * ureg.meter + 4 * ureg.cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47557754477398895 0.44899658684900057 0.6283206792585251 0.8107375491431601 0.44733351515564523 0.3638219255296733 0.7847267992083387 0.4429159119313121 0.6425798692530641 0.8058742757009566] volt\n"
     ]
    }
   ],
   "source": [
    "measures = ureg.Quantity(np.random.random(10), 'volts')\n",
    "print(measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9511550895479779 0.8979931736980011 1.2566413585170502 1.6214750982863202 0.8946670303112905 0.7276438510593466 1.5694535984166773 0.8858318238626242 1.2851597385061282 1.6117485514019132] volt\n"
     ]
    }
   ],
   "source": [
    "print(measures * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<UnitsContainer({'[current]': 1})>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amps = measures / (2 * ureg.ohm)  # I = V/R\n",
    "amps.dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "DimensionalityError",
     "evalue": "Cannot convert from 'volt / ohm' ([current]) to 'second' ([time])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDimensionalityError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-8189ff88b4a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mamps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'seconds'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# DimensionalityError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/python_for_neuroscientists/textbook/venv/lib/python3.8/site-packages/pint/quantity.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, other, *contexts, **ctx_kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_units_container\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_REGISTRY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m         \u001b[0mmagnitude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_magnitude_not_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mctx_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagnitude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/python_for_neuroscientists/textbook/venv/lib/python3.8/site-packages/pint/quantity.py\u001b[0m in \u001b[0;36m_convert_magnitude_not_inplace\u001b[0;34m(self, other, *contexts, **ctx_kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_REGISTRY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_magnitude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_REGISTRY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_magnitude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_magnitude\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mctx_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/python_for_neuroscientists/textbook/venv/lib/python3.8/site-packages/pint/registry.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, value, src, dst, inplace)\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_dimensionality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/python_for_neuroscientists/textbook/venv/lib/python3.8/site-packages/pint/registry.py\u001b[0m in \u001b[0;36m_convert\u001b[0;34m(self, value, src, dst, inplace)\u001b[0m\n\u001b[1;32m   1828\u001b[0m                 \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_magnitude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1830\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_compatible_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_or_system\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/python_for_neuroscientists/textbook/venv/lib/python3.8/site-packages/pint/registry.py\u001b[0m in \u001b[0;36m_convert\u001b[0;34m(self, value, src, dst, inplace)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msrc_offset_unit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdst_offset_unit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m         \u001b[0msrc_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_dimensionality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/python_for_neuroscientists/textbook/venv/lib/python3.8/site-packages/pint/registry.py\u001b[0m in \u001b[0;36m_convert\u001b[0;34m(self, value, src, dst, inplace, check_dimensionality)\u001b[0m\n\u001b[1;32m    978\u001b[0m             \u001b[0;31m# then the conversion cannot be performed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msrc_dim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdst_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mDimensionalityError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0;31m# Here src and dst have only multiplicative units left. Thus we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDimensionalityError\u001b[0m: Cannot convert from 'volt / ohm' ([current]) to 'second' ([time])"
     ]
    }
   ],
   "source": [
    "amps.to('seconds')  # DimensionalityError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some projects this can be a pretty big overkill, but for others this can save many \"silent\" bugs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design vs. Productivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start exercising, one important note to remember: There's a thin line between under- and over-engineering. Very small scripting projects require almost no engineering at all. This might mean that after you gain a few extra months of experience in Python, the structure of code for a small scripting job in Python might be obvious for you right from the get-go. You'll know which data structures you'll have, whether or not you'll need a class or two, and how the user interface might go.\n",
    "\n",
    "On the other hand, large applications which span at least a few thousands lines of code will always need _some_ form of pre-planning. It would be senseless not to write out a diagram of the main modules in your code and their interfaces. One can consider this to be common knowledge, or a simple programmer's instinct. Just like architects sit down and plan for months in advance the construct what they're about to create, programmers should spell out the architecture of their own programs. In no way will this guarantee you'll get the architecture right in the first time, but the design might serve as good building blocks when you start the refactoring process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems mostly occur when you write medium-sized scripts, up to a couple thousand lines. These scripts usually start out small - a few functions that deal with file I/O and display of data - but can grow quite quickly once you start adding functionality. When the script was short you probably didn't even write tests, since you were sure you're handling some insignificant piece of code, and now it starts biting back at you.\n",
    "\n",
    "It's hard to write rules for these occasions. When someone asks me for improved functionality on some short script I wrote, I sometimes tell them it will take more time than I think it should, since I want to devote time to refactor the code, add tests and make the new functionality feel more natural inside it.\n",
    "\n",
    "It's also good practice to use classes to bind data and methods, even when you think they might be an overkill. It's much easier to expand the functionality of classes than of an assortment of functions."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
